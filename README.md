# fixtergeek-mcp-server

[![npm version](https://img.shields.io/npm/v/fixtergeek-mcp-server.svg?style=flat-square)](https://www.npmjs.com/package/fixtergeek-mcp-server)
[![npm downloads](https://img.shields.io/npm/dm/fixtergeek-mcp-server.svg?style=flat-square)](https://www.npmjs.com/package/fixtergeek-mcp-server)

Servidor MCP (Model Context Protocol) modular y extensible con integraci√≥n de LLMs para React y otras aplicaciones.

## üöÄ Caracter√≠sticas

- ‚úÖ **Servidor HTTP completo** con endpoints REST
- ‚úÖ **Integraci√≥n con LLMs** (OpenAI, Ollama, Anthropic)
- ‚úÖ **Sistema de recursos y herramientas** extensible
- ‚úÖ **CORS configurable** para aplicaciones web
- ‚úÖ **Logging configurable** con diferentes niveles
- ‚úÖ **TypeScript** con tipos completos
- ‚úÖ **Fallback a respuestas simuladas** cuando no hay LLM
- ‚úÖ **Tests completos** con Vitest

## üì¶ Instalaci√≥n

```bash
npm install fixtergeek-mcp-server
```

## üîß Uso B√°sico

### Servidor Simple (Sin LLM)

```typescript
import { createMCPServer } from "fixtergeek-mcp-server";

const server = createMCPServer({
  port: 3001,
  cors: true,
});

server.start().then(() => {
  console.log("üöÄ Servidor MCP iniciado");
});
```

### Con Integraci√≥n de LLM

```typescript
import { createMCPServer, exampleConfigs } from "fixtergeek-mcp-server";

// Con OpenAI
const server = createMCPServer({
  ...exampleConfigs.openai,
  llm: {
    provider: "openai",
    apiKey: process.env.OPENAI_API_KEY || "",
    model: "gpt-3.5-turbo",
  },
});

server.start();
```

## üîå Endpoints Disponibles

### GET `/`

Endpoint de prueba

```bash
curl http://localhost:3001/
```

### GET `/resource?uri=<uri>`

Lee un recurso por URI

```bash
curl "http://localhost:3001/resource?uri=file:///hello.txt"
```

### POST `/tool`

Ejecuta una herramienta

```bash
curl -X POST http://localhost:3001/tool \
  -H "Content-Type: application/json" \
  -d '{"tool": "tool-info"}'
```

### POST `/query`

Procesa una consulta del usuario

```bash
curl -X POST http://localhost:3001/query \
  -H "Content-Type: application/json" \
  -d '{"query": "¬øCu√°l es la hora actual?"}'
```

## ü§ñ Integraci√≥n con LLMs

### OpenAI (GPT)

```typescript
import { createMCPServer } from "fixtergeek-mcp-server";

const server = createMCPServer({
  port: 3001,
  llm: {
    provider: "openai",
    apiKey: process.env.OPENAI_API_KEY || "",
    model: "gpt-3.5-turbo",
    temperature: 0.7,
    maxTokens: 1000,
  },
});
```

### Ollama (Local)

```typescript
const server = createMCPServer({
  port: 3001,
  llm: {
    provider: "ollama",
    baseUrl: "http://localhost:11434",
    model: process.env.OLLAMA_MODEL || "llama2",
    temperature: 0.7,
  },
});
```

## üõ†Ô∏è Configuraci√≥n Avanzada

### Registrando Recursos Personalizados

```typescript
import { MCPHttpServer } from "fixtergeek-mcp-server";

const server = new MCPHttpServer({ port: 3001 });

// Registrar un recurso personalizado
server.registerResource(
  "mi-archivo",
  "file:///mi-archivo.txt",
  { description: "Mi archivo personalizado" },
  async () => ({
    success: true,
    data: {
      content: "Contenido de mi archivo",
      mimeType: "text/plain",
    },
    timestamp: Date.now(),
  })
);
```

### Registrando Herramientas Personalizadas

```typescript
// Registrar una herramienta personalizada
server.registerTool(
  "mi-herramienta",
  { description: "Mi herramienta personalizada" },
  async (params) => ({
    success: true,
    data: {
      result: {
        message: "Herramienta ejecutada",
        params,
        timestamp: new Date().toISOString(),
      },
    },
    timestamp: Date.now(),
  })
);
```

## üìã Configuraciones de Ejemplo

```typescript
import { exampleConfigs } from "fixtergeek-mcp-server";

// OpenAI
const openaiConfig = exampleConfigs.openai;

// Ollama
const ollamaConfig = exampleConfigs.ollama;

// Solo simulado
const simulatedConfig = exampleConfigs.simulated;
```

## üîç Logs y Diagn√≥stico

### Niveles de Log

```typescript
const server = createMCPServer({
  port: 3001,
  logLevel: "debug", // 'debug' | 'info' | 'warn' | 'error'
});
```

### Verificar Estado

```typescript
console.log("Servidor corriendo:", server.isServerRunning());
console.log("Recursos disponibles:", server.getAvailableResources());
console.log("Herramientas disponibles:", server.getAvailableTools());
```

## üß™ Pruebas

### Ejecutar Tests

```bash
# Ejecutar tests en modo watch
npm test

# Ejecutar tests una vez
npm run test:run

# Ejecutar tests con coverage
npm run test:coverage
```

### Verificar Servidor

```bash
# Verificar que el servidor est√© funcionando
curl http://localhost:3001/

# Probar una consulta
curl -X POST http://localhost:3001/query \
  -H "Content-Type: application/json" \
  -d '{"query": "hola"}'
```

### Verificar LLM

```bash
# Para OpenAI
curl -H "Authorization: Bearer $OPENAI_API_KEY" \
  https://api.openai.com/v1/models

# Para Ollama
curl http://localhost:11434/api/tags
```

## üîß Variables de Entorno

```bash
# Para OpenAI
export OPENAI_API_KEY="tu-api-key-aqui"

# Para Ollama (instalar primero)
curl -fsSL https://ollama.ai/install.sh | sh
ollama run llama2
```

## üìö API Completa

### MCPServer

Clase base abstracta para servidores MCP.

```typescript
abstract class MCPServer {
  registerResource(
    name: string,
    uri: string,
    metadata: Record<string, any>,
    handler: () => Promise<MCPResourceResponse>
  ): void;
  registerTool(
    name: string,
    metadata: Record<string, any>,
    handler: (params?: any) => Promise<MCPToolResponse>
  ): void;
  readResource(uri: string): Promise<MCPResourceResponse>;
  callTool(name: string, params?: any): Promise<MCPToolResponse>;
  processUserQuery(query: string, context?: any): Promise<MCPResponse>;
  getAvailableResources(): string[];
  getAvailableTools(): string[];
  getConfig(): MCPServerConfig;
  isServerRunning(): boolean;
  abstract start(): Promise<void>;
  abstract stop(): Promise<void>;
}
```

### MCPHttpServer

Implementaci√≥n HTTP del servidor MCP.

```typescript
class MCPHttpServer extends MCPServer {
  constructor(config: MCPServerConfig = {});
  start(): Promise<void>;
  stop(): Promise<void>;
}
```

### LLMFactory

Factory para crear proveedores de LLM.

```typescript
class LLMFactory {
  static createProvider(config: LLMConfig): LLMProvider;
}
```

## üöÄ Scripts Disponibles

```bash
# Desarrollo
npm run dev          # Build en modo watch
npm run build        # Build de producci√≥n
npm run clean        # Limpiar dist

# Tests
npm test             # Tests en modo watch
npm run test:run     # Tests una vez
npm run test:coverage # Tests con coverage

# Linting
npm run lint         # ESLint
npm run type-check   # TypeScript check

# Git
npm run git:push     # Add, commit y push autom√°tico

# NPM Publish
npm run publish:patch # Publicar con bump de patch (0.0.7 ‚Üí 0.0.8)
npm run publish:minor # Publicar con bump de minor (0.0.7 ‚Üí 0.1.0)
npm run publish:major # Publicar con bump de major (0.0.7 ‚Üí 1.0.0)
npm run publish:custom # Publicar con versi√≥n personalizada
```

### Uso del script de Git

```bash
# Usando npm script
npm run git:push "feat: add new feature"

# Usando el script directamente
./scripts/git-push.sh "fix: resolve bug in server"

# Ejemplos de mensajes de commit
npm run git:push "feat: add dynamic Ollama model support"
npm run git:push "docs: update README with npm badges"
npm run git:push "fix: resolve TypeScript compilation errors"
npm run git:push "test: add comprehensive test coverage"
```

### Uso del script de NPM Publish

```bash
# Publicar con bump autom√°tico de patch
npm run publish:patch

# Publicar con bump autom√°tico de minor
npm run publish:minor

# Publicar con bump autom√°tico de major
npm run publish:major

# Publicar con versi√≥n personalizada
npm run publish:custom 0.0.8

# Usando el script directamente
./scripts/npm-publish.sh 0.1.0
```

### üéØ Caracter√≠sticas del script de publicaci√≥n:

- ‚úÖ **Validaciones autom√°ticas**: Git repo, login npm, linting, tests, build
- ‚úÖ **Gesti√≥n de versiones**: Bump autom√°tico o versi√≥n personalizada
- ‚úÖ **Confirmaciones interactivas**: Te pregunta antes de publicar
- ‚úÖ **Commit autom√°tico**: Hace commit de cambios pendientes
- ‚úÖ **Push autom√°tico**: Sube los cambios al repositorio
- ‚úÖ **Mensajes informativos**: Con colores y emojis
- ‚úÖ **Manejo de errores**: Detiene el proceso si algo falla

## ü§ù Contribuir

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/amazing-feature`)
3. Commit tus cambios (`git commit -m 'Add amazing feature'`)
4. Push a la rama (`git push origin feature/amazing-feature`)
5. Abre un Pull Request

## üìÑ Licencia

MIT License - ver [LICENSE](LICENSE) para detalles.

## üîó Enlaces

- [Repositorio](https://github.com/blissito/MCP_Typescript_SDK_Server)
- [Documentaci√≥n completa](https://github.com/blissito/MCP_Typescript_SDK_Server#readme)
- [Issues](https://github.com/blissito/MCP_Typescript_SDK_Server/issues)
